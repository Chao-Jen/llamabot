{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My goal with this notebook is to create the functionality needed to enable QueryBot accept more than just a collection of text files, but instead a collection of arbitrary documents that can be loaded by LlamaHub."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From all of the prototyping down below, it looks like what I need are the following components:\n",
    "\n",
    "1. A mapper for file extension to LlamaHub data loader.\n",
    "2. A function that takes in a file path and returns a list of Document objects,\n",
    "3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from llama_index import download_loader\n",
    "from pyprojroot import here\n",
    "\n",
    "MarkdownReader = download_loader(\"MarkdownReader\")\n",
    "\n",
    "loader = MarkdownReader()\n",
    "documents = loader.load_data(file=here() / Path(\"docs/index.md\"))\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDFReader = download_loader(\"PDFReader\")\n",
    "loader = PDFReader()\n",
    "loaded_docs = loader.load_data(file=here() / \"data/dshiring.pdf\")\n",
    "loaded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "final_docs = []\n",
    "for doc in loaded_docs:\n",
    "    final_docs.extend(split_document(doc))\n",
    "final_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a function that takes in a path and a file extension\n",
    "from llamabot.file_finder import recursive_find\n",
    "from pyprojroot import here\n",
    "from llama_index import download_loader\n",
    "from pathlib import Path\n",
    "\n",
    "python_files = recursive_find(here(), \".py\")\n",
    "markdown_files = recursive_find(here(), \".md\")\n",
    "# jupyter_files = recursive_find(here(), \".ipynb\")\n",
    "pdf_files = recursive_find(here(), \".pdf\")\n",
    "\n",
    "wanted_files = [] + markdown_files + pdf_files\n",
    "\n",
    "# Step 0: map file extensions to llamahub loaders\n",
    "extension_loader_mapping = {\n",
    "    \".pdf\": \"PDFReader\",\n",
    "    \".docx\": \"DocxReader\",\n",
    "    \".pptx\": \"PptxReader\",\n",
    "    \".xlsx\": \"PandasExcelReader\",\n",
    "}\n",
    "\n",
    "# Step 1: Use the appropriate document loader to load the document.\n",
    "# loaded_docs are named as such because they are loaded from llamahub loaders.\n",
    "# however, we still will need to split them up further into chunks of 2,000 tokens,\n",
    "# which will be done later to give us `final_docs`.\n",
    "\n",
    "\n",
    "def magic_load_doc(file_path) -> List[Document]:\n",
    "    loader_string: str = extension_loader_mapping.get(Path(file_path).suffix, None)\n",
    "    if loader_string is not None:\n",
    "        # Treat this as a document that needs special processing.\n",
    "        Loader = download_loader(loader_string)\n",
    "        loader = Loader()\n",
    "        documents = loader.load_data(file_path)\n",
    "\n",
    "    else:\n",
    "        # Treat this as a plain text file.\n",
    "        with open(file_path, \"r+\") as f:\n",
    "            documents = [Document(text=str(file_path) + f.read())]\n",
    "    return documents\n",
    "\n",
    "\n",
    "raw_docs = []\n",
    "for file in wanted_files:\n",
    "    raw_docs.extend(magic_load_doc(file))\n",
    "\n",
    "# Step 2: Ensure each doc is 2000 tokens long maximum.\n",
    "final_docs = []\n",
    "for doc in loaded_docs:\n",
    "    final_docs.extend(split_document(doc))\n",
    "final_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Combine all of the documents into a single GPTIndex\n",
    "from pathlib import Path\n",
    "from typing import List, Union\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from llama_index import Document, GPTSimpleVectorIndex, LLMPredictor, ServiceContext\n",
    "from llama_index.response.schema import Response\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.callbacks.base import CallbackManager\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model_name=\"gpt-4\",\n",
    "    temperature=0.0,\n",
    "    streaming=True,\n",
    "    verbose=True,\n",
    "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    ")\n",
    "llm_predictor = LLMPredictor(llm=chat)\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n",
    "index = GPTSimpleVectorIndex.from_documents(final_docs, service_context=service_context)\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.query(\"What tecnical skills do we need to cover when hiring data scientists?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamabot import ChatBot\n",
    "\n",
    "\n",
    "programmer = ChatBot(\"You are a highly skilled Python programmer.\")\n",
    "\n",
    "\n",
    "programmer(\n",
    "    \"Write me a function that takes in a path to a source code repository, amd retirms a list of the paths to Python source files.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "programmer(\n",
    "    \"Can you improve the code such that it errors out if the path to the repository is not an actual git repo?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamabot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
