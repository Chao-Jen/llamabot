{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamabot import ChatBot  # just to enable openai key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 1: Read in a collection of files and pass them to the correct loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "import glob \n",
    "from pyprojroot import here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_read = glob.glob(str(here() / \"data\" / \"blog\") + \"**/**/*.lr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import GPTSimpleVectorIndex, SimpleDirectoryReader\n",
    "from pathlib import Path \n",
    "from typing import List, Union\n",
    "from llama_index import LLMPredictor, ServiceContext, Document\n",
    "from langchain.chat_models import ChatOpenAI \n",
    "from langchain.schema import SystemMessage, HumanMessage \n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "class QueryBot:\n",
    "    def __init__(self, system_message: str, doc_paths: List[Union[str, Path]] = None, saved_index_path: Union[str, Path] = None):\n",
    "        \"\"\"Initialize QueryBot.\n",
    "\n",
    "        Pass in either the doc_paths or saved_index_path to initialize the QueryBot.\n",
    "\n",
    "        QueryBot is not designed to have memory.\n",
    "\n",
    "        Underneath the hood, we \n",
    "\n",
    "        :param system_message: The system message to send to the chatbot.\n",
    "        :param doc_paths: A list of paths to the documents to use for the chatbot.\n",
    "            These are assumed to be plain text files.\n",
    "        :param saved_index_path: The path to the saved index to use for the chatbot.\n",
    "        \"\"\"\n",
    "\n",
    "        self.system_message = system_message\n",
    "\n",
    "        if saved_index_path is not None:\n",
    "            self.index = GPTSimpleVectorIndex.load_from_disk(saved_index_path)\n",
    "\n",
    "        else:\n",
    "            self.doc_paths = doc_paths\n",
    "            splitter = TokenTextSplitter(chunk_size=2000, chunk_overlap=0)\n",
    "            documents = []\n",
    "            for fname in doc_paths:\n",
    "                with open(fname, 'r') as f:\n",
    "                    docs = splitter.split_text(f.read())\n",
    "                    documents.extend([Document(d) for d in docs])\n",
    "            chat = ChatOpenAI(model_name=\"gpt-4\", temperature=0.5)\n",
    "            llm_predictor = LLMPredictor(llm=chat)\n",
    "            service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n",
    "            index = GPTSimpleVectorIndex.from_documents(documents, service_context=service_context)\n",
    "            self.index = index \n",
    "\n",
    "    def __call__(self, query: str, **kwargs) -> str:\n",
    "        q = \"\"\n",
    "        q += self.system_message + \"\\n\\n\"\n",
    "        q += query + \"\\n\\n\"\n",
    "        result = self.index.query(q, **kwargs)\n",
    "        return result\n",
    "\n",
    "\n",
    "    def save(self, path: Union[str, Path]):\n",
    "        self.index.save_to_disk(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = QueryBot(system_message=\"You are a Q&A bot.\", doc_paths=files_to_read[0:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = bot(\"Do you have any advice for me on career development?\", similarity_top_k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(result.response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.source_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot.save(here() / \"data\" / \"blog_index.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_paths = files_to_read\n",
    "\n",
    "splitter = TokenTextSplitter(chunk_size=200, chunk_overlap=50)\n",
    "documents = []\n",
    "for fname in doc_paths:\n",
    "    with open(fname, 'r') as f:\n",
    "        docs = splitter.split_text(f.read())\n",
    "        documents.extend([Document(d) for d in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = QueryBot(system_message=\"You are a Q&A bot.\", doc_paths=files_to_read[0:50])\n",
    "# bot(\"Do you have any advice for me on career development?\", similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "splitter = TokenTextSplitter(encoding_name=\"gpt2\")\n",
    "splitter.split_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamabot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
