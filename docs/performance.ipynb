{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query performance with different configurations\n",
    "\n",
    "There are many ways to query a collection of documents.\n",
    "Configuring the query different ways can result in different quality of results.\n",
    "The time taken will vary as well.\n",
    "The kinds of things we need to consider are:\n",
    "\n",
    "1. How long each \"document\" is --> text splitting.\n",
    "2. When querying, how many documents do we consider as context for answering the query?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test collection of documents that I will use is my blog's collection of posts.\n",
    "I will vary the following parameters:\n",
    "\n",
    "1. Chunk size when loading documents.\n",
    "2. K - the number of documents to pass into the response synthesis module,\n",
    "\n",
    "What I'll be doing is qualitatively judging the response,\n",
    "and measuring the time it took to generate the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import Document, GPTSimpleVectorIndex\n",
    "from llama_index.docstore import DocumentStore\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from llamabot.bot import openai #just to run the environment variables code\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-4\", temperature=0.0)\n",
    "chat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyprojroot import here\n",
    "import glob \n",
    "from pathlib import Path \n",
    "\n",
    "blog_contents = glob.glob(str(here()) + \"/data/blog/**/*.lr\")\n",
    "pngs = glob.glob(str(here()) + \"/data/blog/**/*.png\")\n",
    "jpgs = glob.glob(str(here()) + \"/data/blog/**/*.jpg\")\n",
    "jpegs = glob.glob(str(here()) + \"/data/blog/**/*.jpeg\")\n",
    "pdfs = glob.glob(str(here()) + \"/data/blog/**/*.pdf\")\n",
    "ais = glob.glob(str(here()) + \"/data/blog/**/*.ai\")\n",
    "\n",
    "delete = [] + pngs + jpgs + ais  + jpegs + pdfs\n",
    "\n",
    "for file in delete:\n",
    "    Path(file).unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_blogpost(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.read()\n",
    "    return lines"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split text by using Markdown Text Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import MarkdownTextSplitter, TokenTextSplitter\n",
    "from llama_index import LLMPredictor, ServiceContext\n",
    "\n",
    "llm_predictor = LLMPredictor(llm=chat)\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n",
    "\n",
    "\n",
    "# Read each blog post, split according to the MarkdownTextSplitter, and\n",
    "# cast it back into the LlamaIndex Document format.\n",
    "\n",
    "blog_posts = []\n",
    "splitter = TokenTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "for f in blog_contents:\n",
    "    blog_post = read_blogpost(f)\n",
    "    chunks = splitter.split_text(blog_post)\n",
    "    blog_posts.extend([Document(chunk) for chunk in chunks])\n",
    "len(blog_posts)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done with GPT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = GPTSimpleVectorIndex.from_documents(blog_posts, service_context=service_context)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takes about 23 seconds with GPT4 to build a vector index."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take top 1 node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_1 = vector_index.query(\"How do you think about career development?\", response_mode=\"default\")\n",
    "result_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(result_1.response))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very fast, ~15 seconds to answer, including API call latency."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take top 3 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_3 = vector_index.query(\"How do you think about career development?\", response_mode=\"default\", similarity_top_k=3)\n",
    "result_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(result_3.response))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Took about 90 seconds."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take top 5 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_5 = vector_index.query(\"How do you think about career development?\", response_mode=\"default\", similarity_top_k=5)\n",
    "result_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(result_5.response))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Took about 3 minutes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do with GPT-3 (default in LlamaIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = GPTSimpleVectorIndex.from_documents(blog_posts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_1 = vector_index.query(\"How do you think about career development?\", response_mode=\"default\")\n",
    "display(Markdown(result_1.response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_3 = vector_index.query(\"How do you think about career development?\", response_mode=\"default\", similarity_top_k=3)\n",
    "display(Markdown(result_3.response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_5 = vector_index.query(\"How do you think about career development?\", response_mode=\"default\", similarity_top_k=5)\n",
    "display(Markdown(result_5.response))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My thoughts so far: \n",
    "\n",
    "1. This speed is not suited to a chat bot.\n",
    "2. It can, however, be used for an email bot (which I've been building secretly!).\n",
    "3. If we don't use GPT4, the response synthesis quality is much worse. \n",
    "4. If we use GPT4, the response synthesis time is much slower."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Async Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_1_async = vector_index.aquery(\"How do you think about career development?\", response_mode=\"default\")\n",
    "await result_1_async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import GPTTreeIndex\n",
    "tree_index = GPTTreeIndex.from_documents(blog_posts)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tree index took about 4 minutes to build, with lots of tokens used up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tree_index.query(\"How do you think about career development?\", response_mode=\"default\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamabot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
