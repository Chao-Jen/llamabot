{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache prototype\n",
    "\n",
    "from llamabot.doc_processor import split_document, magic_load_doc\n",
    "\n",
    "from pyprojroot import here\n",
    "\n",
    "fpath = here() / \"data/return_label.pdf\"\n",
    "\n",
    "document = magic_load_doc(fpath)\n",
    "\n",
    "split_docs = split_document(document[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate hash of the file.\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def compute_file_hash(fpath: Path) -> str:\n",
    "    file_content = fpath.read_bytes()\n",
    "    return hashlib.sha256(file_content).hexdigest()\n",
    "\n",
    "\n",
    "file_hash = compute_file_hash(fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one GPTVectorStoreIndex per file.\n",
    "from llama_index import GPTVectorStoreIndex, LLMPredictor, ServiceContext\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks.base import BaseCallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "\n",
    "def make_service_context():\n",
    "    chat = ChatOpenAI(\n",
    "        model_name=\"gpt-4\",\n",
    "        temperature=0.0,\n",
    "        streaming=True,\n",
    "        verbose=True,\n",
    "        callback_manager=BaseCallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "    )\n",
    "    llm_predictor = LLMPredictor(llm=chat)\n",
    "    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n",
    "    return service_context\n",
    "\n",
    "\n",
    "service_context = make_service_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.storage.index_store import SimpleIndexStore\n",
    "from llama_index.vector_stores import SimpleVectorStore\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from pathlib import Path\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import load_index_from_storage\n",
    "\n",
    "\n",
    "def get_persist_dir(file_hash: str):\n",
    "    persist_dir = Path.home() / \".llamabot\" / \"cache\" / file_hash\n",
    "    return persist_dir\n",
    "\n",
    "\n",
    "def load_index(persist_dir, service_context):\n",
    "    storage_context = StorageContext.from_defaults(\n",
    "        docstore=SimpleDocumentStore.from_persist_dir(persist_dir=persist_dir),\n",
    "        vector_store=SimpleVectorStore.from_persist_dir(persist_dir=persist_dir),\n",
    "        index_store=SimpleIndexStore.from_persist_dir(persist_dir=persist_dir),\n",
    "    )\n",
    "    index = load_index_from_storage(storage_context, service_context=service_context)\n",
    "    if index.summary == \"None\":\n",
    "        index.summary = index.as_query_engine().query(\"Summarize this document.\")\n",
    "        index.storage_context.persist(persist_dir=persist_dir)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_storage_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_index(docs, persist_dir, service_context):\n",
    "    # create parser and parse document into nodes\n",
    "    parser = SimpleNodeParser()\n",
    "    nodes = parser.get_nodes_from_documents(docs)\n",
    "\n",
    "    # create (or load) docstore and add nodes\n",
    "    storage_context = StorageContext.from_defaults(\n",
    "        docstore=SimpleDocumentStore(),\n",
    "        vector_store=SimpleVectorStore(),\n",
    "        index_store=SimpleIndexStore(),\n",
    "    )\n",
    "    storage_context.docstore.add_documents(nodes)\n",
    "\n",
    "    index = GPTVectorStoreIndex(\n",
    "        nodes,\n",
    "        storage_context=storage_context,\n",
    "        index_id=file_hash,\n",
    "        service_context=service_context,\n",
    "    )\n",
    "    index.summary = index.as_query_engine().query(\"Summarize this document.\")\n",
    "    index.storage_context.persist(persist_dir=persist_dir)\n",
    "    return index\n",
    "\n",
    "\n",
    "def make_or_load_index(fpath):\n",
    "    file_hash = compute_file_hash(fpath)\n",
    "    service_context = make_service_context()\n",
    "    persist_dir = get_persist_dir(file_hash)\n",
    "\n",
    "    if persist_dir.exists():\n",
    "        index = load_index(persist_dir, service_context=service_context)\n",
    "    else:\n",
    "        persist_dir.mkdir(exist_ok=True, parents=True)\n",
    "        index = make_index(split_docs, persist_dir, service_context=service_context)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import load_index_from_storage, ComposableGraph\n",
    "\n",
    "fpaths = [\n",
    "    here() / \"data/return_label.pdf\",\n",
    "    here() / \"data/dshiring.pdf\",\n",
    "]\n",
    "\n",
    "hashes = list(map(compute_file_hash, fpaths))\n",
    "persist_dirs = list(map(get_persist_dir, hashes))\n",
    "\n",
    "# index = load_index(persist_dir, service_context=service_context)\n",
    "indexes = [\n",
    "    load_index(persist_dir, service_context=service_context)\n",
    "    for persist_dir in persist_dirs\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes[1].summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes[0].summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamabot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
