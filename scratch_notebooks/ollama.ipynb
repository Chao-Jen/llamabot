{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamabot import SimpleBot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bot = SimpleBot(\"You are Richard Feynman\", model_name=\"orca-mini\", verbose=True)\n",
    "# bot(\"Tell me about black holes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, looks like the thing works with both verbose = True and verbose = False!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try writing commit diff\n",
    "from llamabot import SimpleBot\n",
    "from llamabot.prompt_manager import prompt\n",
    "\n",
    "bot = SimpleBot(\n",
    "    \"You are an expert user of Git.\",\n",
    "    # model_name=os.getenv(\"OPENAI_DEFAULT_MODEL\", \"gpt-3.5-turbo-16k-0613\"),\n",
    "    model_name=\"codellama:13b\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = 'diff --git a/llamabot/bot/model_dispatcher.py b/llamabot/bot/model_dispatcher.py\\nindex eab00d5..abe4d70 100644\\n--- a/llamabot/bot/model_dispatcher.py\\n+++ b/llamabot/bot/model_dispatcher.py\\n@@ -12,6 +12,7 \\n@@ from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\\n from langchain.callbacks.base import BaseCallbackManager\\n from time import sleep\\n from loguru import logger\\n+from functools import partial\\n \\n # get this list from: https://ollama.ai/library\\n ollama_model_keywords = [\\n@@ -62,13 +63,14 @@ def create_model(\\n     :param verbose: (LangChain config) Whether to print debug messages.\\n     :return: The model.\\n     \"\"\"\\n-    ModelClass = ChatOpenAI\\n+    # We use a `partial` here to ensure that we have the correct way of specifying\\n+    # a model name between ChatOpenAI and ChatOllama.\\n+    ModelClass = partial(ChatOpenAI, model_name=model_name)\\n     if model_name.split(\":\")[0] in ollama_model_keywords:\\n-        ModelClass = ChatOllama\\n+        ModelClass = partial(ChatOllama, model=model_name)\\n         launch_ollama(model_name, verbose=verbose)\\n \\n     return ModelClass(\\n-        model_name=model_name,\\n         temperature=temperature,\\n         streaming=streaming,\\n         verbose=verbose,'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llamabot.code_manipulation import get_git_diff\n",
    "# from pyprojroot import here\n",
    "\n",
    "# diff = get_git_diff(here())\n",
    "\n",
    "# print(diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@prompt\n",
    "def compose_commit_message(diff, template):\n",
    "    \"\"\"Here is a git diff:\n",
    "\n",
    "    {{ diff }}\n",
    "\n",
    "    Here is the template for writing commit messages:\n",
    "\n",
    "    {{ template }}\n",
    "\n",
    "    Please compose a commit message for the provided diff in the format of the template.\n",
    "    Return only the commit message and nothing else.\n",
    "\n",
    "    [AI]: Here is a possible commit message for the provided diff:\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "template = \"\"\"\n",
    "<type>[optional scope]: <description>\n",
    "\n",
    "[body describing in detail the changes made to the code]\n",
    "\n",
    "[optional footer(s)]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "bot(compose_commit_message(diff, template))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so I have new documented behaviour.\n",
    "\n",
    "If:\n",
    "\n",
    "1. The Ollama model server is running (e.g. Ollama app on macOS),\n",
    "2. The Ollama model has been downloaded,\n",
    "\n",
    "Then:\n",
    "\n",
    "1. LangChain will autolaunch the Ollama model API server (I think?)\n",
    "2. I don't need to magically launch the process within LlamaBot.\n",
    "\n",
    "Ok, that's neat, and I think I like this better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Dispatch\"\"\"\n",
    "import contextvars\n",
    "\n",
    "from langchain.callbacks.base import BaseCallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chat_models import ChatOllama, ChatOpenAI\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "from llamabot.config import default_language_model\n",
    "from llamabot.recorder import autorecord\n",
    "\n",
    "prompt_recorder_var = contextvars.ContextVar(\"prompt_recorder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"codellama\"\n",
    "temperature = 0.0\n",
    "verbose = True\n",
    "streaming = True\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name=model_name,\n",
    "    temperature=temperature,\n",
    "    streaming=streaming,\n",
    "    verbose=verbose,\n",
    "    callback_manager=BaseCallbackManager(\n",
    "        handlers=[StreamingStdOutCallbackHandler()] if streaming else []\n",
    "    ),\n",
    ")\n",
    "\n",
    "llm()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dispatch_model(\n",
    "    model_name,\n",
    "    temperature=0.0,\n",
    "    streaming=True,\n",
    "    verbose=True,\n",
    "):\n",
    "    \"\"\"Dispatch and create the right model\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class SimpleBot:\n",
    "    \"\"\"Simple Bot that is primed with a system prompt, accepts a human message,\n",
    "    and sends back a single response.\n",
    "\n",
    "    This bot does not retain chat history.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        system_prompt,\n",
    "        temperature=0.0,\n",
    "        model_name=\"codellama\",\n",
    "        streaming=True,\n",
    "        verbose=True,\n",
    "    ):\n",
    "        \"\"\"Initialize the SimpleBot.\n",
    "\n",
    "        :param system_prompt: The system prompt to use.\n",
    "        :param temperature: The model temperature to use.\n",
    "            See https://platform.openai.com/docs/api-reference/completions/create#completions/create-temperature\n",
    "            for more information.\n",
    "        :param model_name: The name of the OpenAI model to use.\n",
    "        :param streaming: (LangChain config) Whether to stream the output to stdout.\n",
    "        :param verbose: (LangChain config) Whether to print debug messages.\n",
    "        \"\"\"\n",
    "        self.system_prompt = system_prompt\n",
    "        self.model = ChatOllama(\n",
    "            model_name=model_name,\n",
    "            temperature=temperature,\n",
    "            streaming=streaming,\n",
    "            verbose=verbose,\n",
    "            callback_manager=BaseCallbackManager(\n",
    "                handlers=[StreamingStdOutCallbackHandler()] if streaming else []\n",
    "            ),\n",
    "        )\n",
    "        self.chat_history = []\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def __call__(self, human_message: str) -> AIMessage:\n",
    "        \"\"\"Call the SimpleBot.\n",
    "\n",
    "        :param human_message: The human message to use.\n",
    "        :return: The response to the human message, primed by the system prompt.\n",
    "        \"\"\"\n",
    "\n",
    "        messages = [\n",
    "            SystemMessage(content=self.system_prompt),\n",
    "            HumanMessage(content=human_message),\n",
    "        ]\n",
    "        response = self.model(messages)\n",
    "        self.chat_history.append(HumanMessage(content=human_message))\n",
    "        self.chat_history.append(response)\n",
    "        autorecord(human_message, response.content)\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = SimpleBot(\"You are a coding expert\", model_name=\"codellama\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot(\"how do you write a FastAPI endpoint?\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamabot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
