{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamabot import SimpleBot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = SimpleBot(\"You are Richard Feynman\", model_name='codellama')\n",
    "bot(\"how are you doing?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Dispatch\"\"\"\n",
    "import contextvars\n",
    "\n",
    "from langchain.callbacks.base import BaseCallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chat_models import ChatOllama, ChatOpenAI\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "from llamabot.config import default_language_model\n",
    "from llamabot.recorder import autorecord\n",
    "\n",
    "prompt_recorder_var = contextvars.ContextVar(\"prompt_recorder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"codellama\"\n",
    "temperature = 0.0\n",
    "verbose = True\n",
    "streaming = True\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name=model_name,\n",
    "    temperature=temperature,\n",
    "    streaming=streaming,\n",
    "    verbose=verbose,\n",
    "    callback_manager=BaseCallbackManager(\n",
    "        handlers=[StreamingStdOutCallbackHandler()] if streaming else []\n",
    "    ),\n",
    ")\n",
    "\n",
    "llm()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dispatch_model(\n",
    "    model_name,\n",
    "    temperature=0.0,\n",
    "    streaming=True,\n",
    "    verbose=True,\n",
    "):\n",
    "    \"\"\"Dispatch and create the right model\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class SimpleBot:\n",
    "    \"\"\"Simple Bot that is primed with a system prompt, accepts a human message,\n",
    "    and sends back a single response.\n",
    "\n",
    "    This bot does not retain chat history.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        system_prompt,\n",
    "        temperature=0.0,\n",
    "        model_name=\"codellama\",\n",
    "        streaming=True,\n",
    "        verbose=True,\n",
    "    ):\n",
    "        \"\"\"Initialize the SimpleBot.\n",
    "\n",
    "        :param system_prompt: The system prompt to use.\n",
    "        :param temperature: The model temperature to use.\n",
    "            See https://platform.openai.com/docs/api-reference/completions/create#completions/create-temperature\n",
    "            for more information.\n",
    "        :param model_name: The name of the OpenAI model to use.\n",
    "        :param streaming: (LangChain config) Whether to stream the output to stdout.\n",
    "        :param verbose: (LangChain config) Whether to print debug messages.\n",
    "        \"\"\"\n",
    "        self.system_prompt = system_prompt\n",
    "        self.model = ChatOllama(\n",
    "            model_name=model_name,\n",
    "            temperature=temperature,\n",
    "            streaming=streaming,\n",
    "            verbose=verbose,\n",
    "            callback_manager=BaseCallbackManager(\n",
    "                handlers=[StreamingStdOutCallbackHandler()] if streaming else []\n",
    "            ),\n",
    "        )\n",
    "        self.chat_history = []\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def __call__(self, human_message: str) -> AIMessage:\n",
    "        \"\"\"Call the SimpleBot.\n",
    "\n",
    "        :param human_message: The human message to use.\n",
    "        :return: The response to the human message, primed by the system prompt.\n",
    "        \"\"\"\n",
    "\n",
    "        messages = [\n",
    "            SystemMessage(content=self.system_prompt),\n",
    "            HumanMessage(content=human_message),\n",
    "        ]\n",
    "        response = self.model(messages)\n",
    "        self.chat_history.append(HumanMessage(content=human_message))\n",
    "        self.chat_history.append(response)\n",
    "        autorecord(human_message, response.content)\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = SimpleBot(\"You are a coding expert\", model_name=\"codellama\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot(\"how do you write a FastAPI endpoint?\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamabot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
