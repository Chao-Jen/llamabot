{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will attempt to systematically compare LlamaBot's QueryBot class\n",
    "using multiscale vs. single scale embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are multiscale embeddings all about?\n",
    "To understand this, I need to walk back and define RAG: Retrieval Augmented Generation.\n",
    "RAG is a technique used in the context of LLMs to synthesize responses to queries.\n",
    "At its basic level, RAG begins by embedding a document into vector space.\n",
    "When embedding the document, we chunk up the text into smaller chunks\n",
    "and store the embeddings inside a vector storage system.\n",
    "Then, we take in a human's query, embed it,\n",
    "and then search for the `k` most similar documents' embeddings to the query,\n",
    "typically measured by cosine similarity.\n",
    "Then, those `k` documents (or a subset of them) are stuffed back into the LLM context\n",
    "for the LLM to generate responses to.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, queries to documents can sometimes be small in nature,\n",
    "involving individual facts about the document\n",
    "that can be stored within a small context.\n",
    "Retrieving a large fragment of the document based on embeddings\n",
    "and stuffing it all into a context window is quite wasteful and limiting.\n",
    "On the other hand, some queries about documents can be quite large,\n",
    "needing to cover multiple sections of the document at one shot.\n",
    "In this case, if our embeddings' source text chunk sizes are large, \n",
    "we may not be able to stuff the full set of associated documents \n",
    "into the language model context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One idea that came to my mind was to try _multi-scale embeddings_.\n",
    "What do we mean here?\n",
    "By that, I mean chunking the source text into different chunks.\n",
    "For example, if the source text is a book,\n",
    "we can chunk it into chapters, paragraphs, and sentences.\n",
    "Then, we can embed each of these chunks into a vector space.\n",
    "When performing a similarity search, \n",
    "the nature of the query should naturally dictate \n",
    "whether a larger chunk needs to be retrieved\n",
    "or if a smaller chunk is needed, or both.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But does this idea work?\n",
    "That's what I'd like to evaluate in this notebook.\n",
    "To test the idea, I will use `QueryBot`'s latest configuration,\n",
    "in which I implemented multiscale embeddings and single-scale embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamabot import QueryBot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The document that we will work with is the paper on CodonBERT. It's long enough and complex enough but also not too expensive. Firstly, I need to load the library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyprojroot import here\n",
    "from llamabot.zotero.library import ZoteroLibrary\n",
    "from pathlib import Path\n",
    "\n",
    "ZOTERO_JSON_DIR = Path.home() / \".llamabot/zotero/zotero_index/\"\n",
    "library = ZoteroLibrary(json_dir=ZOTERO_JSON_DIR)  # this is my pre-cached library\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we get the PDF and load it into two separate instances of the QueryBot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_key = library.key_title_map(inverse=True)[\n",
    "    \"The simplicity of protein sequence-function relationships\"\n",
    "]\n",
    "entry = library[paper_key]\n",
    "fpath = entry.download_pdf(Path(\"/tmp\"))\n",
    "\n",
    "docbot_single = QueryBot(\n",
    "    \"You are an expert in answering questions about a paper.\",\n",
    "    doc_paths=[fpath],\n",
    "    chunk_sizes=[2000],\n",
    ")\n",
    "\n",
    "docbot_multi = QueryBot(\n",
    "    \"You are an expert in answering questions about a paper.\",\n",
    "    doc_paths=[fpath],\n",
    "    chunk_sizes=[200, 500, 1000, 2000, 5000],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = \"Please summarize this paper for me.\"\n",
    "_ = docbot_single(prompt1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = docbot_multi(prompt1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_single = docbot_single.source_nodes[\"Please summarize this paper for me.\"]\n",
    "for node in sources_single:\n",
    "    print(len(node.node.text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_single = docbot_multi.source_nodes[\"Please summarize this paper for me.\"]\n",
    "for node in sources_single:\n",
    "    print(len(node.node.text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2 = \"Please help me explain first and second order effects.\"\n",
    "\n",
    "_ = docbot_single(prompt2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = docbot_multi(prompt2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_single = docbot_single.source_nodes[\"Please summarize this paper for me.\"]\n",
    "for node in sources_single:\n",
    "    print(len(node.node.text))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamabot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
