{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyprojroot import here\n",
    "\n",
    "pdf_path = here() / \"data/JMLR-23-0380-1.pdf\"\n",
    "assert pdf_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from llamabot.config import default_language_model\n",
    "from langchain.callbacks.base import BaseCallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from llama_index import (\n",
    "    GPTVectorStoreIndex,\n",
    "    LLMPredictor,\n",
    "    ServiceContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "\n",
    "# Configuration (copied from QueryBot __init__)\n",
    "model_name = default_language_model()\n",
    "temperature = 0.0\n",
    "stream = True\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model_name=model_name,\n",
    "    temperature=temperature,\n",
    "    streaming=True,\n",
    "    verbose=True,\n",
    "    callback_manager=BaseCallbackManager(\n",
    "        handlers=[StreamingStdOutCallbackHandler()] if stream else []\n",
    "    ),\n",
    ")\n",
    "llm_predictor = LLMPredictor(llm=chat)\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamabot.bot.querybot import make_or_load_index\n",
    "\n",
    "doc_paths = [pdf_path]\n",
    "large_chunk_size = int(2000)\n",
    "small_chunk_size = int(500)\n",
    "chunk_overlap = 0\n",
    "use_cache = True\n",
    "\n",
    "large_index = make_or_load_index(doc_paths, large_chunk_size, chunk_overlap, use_cache)\n",
    "small_index = make_or_load_index(doc_paths, small_chunk_size, chunk_overlap, use_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_similarity_top_k = 5\n",
    "small_similarity_top_k = 20\n",
    "large_retriever = large_index.as_retriever(similarity_top_k=large_similarity_top_k)\n",
    "small_retriever = small_index.as_retriever(similarity_top_k=small_similarity_top_k)\n",
    "\n",
    "query = \"What is Post-training of Feature extractors' algorithm written out explicitly? Translate the symbols into plain English, but retain their original symbols when referring to them.\"\n",
    "\n",
    "large_source_nodes = large_retriever.retrieve(query)\n",
    "large_source_texts = [n.node.text for n in large_source_nodes]\n",
    "\n",
    "small_source_nodes = small_retriever.retrieve(query)\n",
    "small_source_texts = [n.node.text for n in small_source_nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now build the full query that gets stuffed into `chat`:\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "faux_chat_history = []\n",
    "faux_chat_history.append(SystemMessage(content=\"You are a Q&A bot about papers!\"))\n",
    "faux_chat_history.append(\n",
    "    SystemMessage(content=\"Here is the context you will be working with:\")\n",
    ")\n",
    "# for text in small_source_texts:\n",
    "#     faux_chat_history.append(SystemMessage(content=text))\n",
    "\n",
    "for text in large_source_texts:\n",
    "    faux_chat_history.append(SystemMessage(content=text))\n",
    "faux_chat_history.append(HumanMessage(content=query))\n",
    "response = chat(faux_chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now build the full query that gets stuffed into `chat`:\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "faux_chat_history = []\n",
    "faux_chat_history.append(SystemMessage(content=\"You are a Q&A bot about papers!\"))\n",
    "faux_chat_history.append(\n",
    "    SystemMessage(content=\"Here is the context you will be working with:\")\n",
    ")\n",
    "for text in small_source_texts:\n",
    "    faux_chat_history.append(SystemMessage(content=text))\n",
    "\n",
    "# for text in large_source_texts:\n",
    "#     faux_chat_history.append(SystemMessage(content=text))\n",
    "faux_chat_history.append(HumanMessage(content=query))\n",
    "response = chat(faux_chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[n.score for n in small_source_nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[n.score for n in large_source_nodes]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamabot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
