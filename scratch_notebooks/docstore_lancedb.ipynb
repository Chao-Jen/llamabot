{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "\n",
    "uri = \"~/.llamabot/lancedb\"\n",
    "db = lancedb.connect(uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import embedding\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "text_to_embed = [{\"document\": \"Hello world!\"}, {\"document\": \"Hello again!\"}]\n",
    "\n",
    "# response = embedding(\n",
    "#     model=\"text-embedding-3-small\", input=text_to_embed\n",
    "# )  # , api_base=\"http://{os.getenv('OLLAMA_SERVER')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lancedb.embeddings import get_registry\n",
    "from lancedb.pydantic import LanceModel, Vector\n",
    "\n",
    "registry = get_registry()\n",
    "func = registry.get(name=\"sentence-transformers\").create()\n",
    "\n",
    "\n",
    "class DocstoreEntry(LanceModel):\n",
    "    document: str = func.SourceField()\n",
    "    vector: Vector(func.ndims()) = func.VectorField()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this information to the lancedb database\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    table = db.create_table(\"dummy_table\", schema=DocstoreEntry)\n",
    "except Exception as e:\n",
    "    db.drop_table(\"dummy_table\")\n",
    "    table = db.create_table(\"dummy_table\", schema=DocstoreEntry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.add(text_to_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.search().limit(None).to_pydantic(DocstoreEntry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.search(text_to_embed[0][\"document\"]).limit(1).to_pydantic(DocstoreEntry)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# from llamabot.components.docstore import DocumentStore\n",
    "\n",
    "\n",
    "# ds = DocumentStore(collection_name=\"stuff\")\n",
    "# ds.reset()\n",
    "# ds.append(\"Hello!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import lancedb\n",
    "from lancedb.embeddings import get_registry\n",
    "from lancedb.pydantic import LanceModel, Vector\n",
    "from llamabot.doc_processor import magic_load_doc, split_document\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "registry = get_registry()\n",
    "func = registry.get(name=\"sentence-transformers\").create()\n",
    "\n",
    "\n",
    "class LanceDBDocStore:\n",
    "    def __init__(\n",
    "        self,\n",
    "        table_name: str,\n",
    "        storage_path: Path = Path.home() / \".llamabot\" / \"lancedb\",\n",
    "        schema: Optional[LanceModel] = DocstoreEntry,\n",
    "    ):\n",
    "        self.table_name = table_name\n",
    "        self.db = lancedb.connect(storage_path)\n",
    "\n",
    "        try:\n",
    "            self.table = self.db.open_table(table_name)\n",
    "        except FileNotFoundError:\n",
    "            self.table = self.db.create_table(table_name, schema=schema)\n",
    "\n",
    "    def __contains__(self, other: str) -> bool:\n",
    "        \"\"\"Returns boolean whether the 'other' document is in the store.\"\"\"\n",
    "        all_items = self.table.search().limit(None).to_pydantic(DocstoreEntry)\n",
    "        texts = set([item.document for item in all_items])\n",
    "        return other in texts\n",
    "\n",
    "    def append(self, document: str, metadata: dict = {}):\n",
    "        self.table.add([{\"document\": document}])\n",
    "\n",
    "    def extend(self, documents: list[str], metadata: dict = {}):\n",
    "        # self.table.add(documents)\n",
    "        for doc in documents:\n",
    "            self.append(doc)\n",
    "\n",
    "    def retrieve(self, query: str, n_results: int = 10):\n",
    "        results: list[DocstoreEntry] = (\n",
    "            self.table.search(query).limit(n_results).to_pydantic(DocstoreEntry)\n",
    "        )\n",
    "        return [r.document for r in results]\n",
    "\n",
    "    def reset(self):\n",
    "        self.db.drop_table(self.table_name)\n",
    "        self.table = self.db.create_table(self.table_name, schema=DocstoreEntry)\n",
    "\n",
    "    def add_documents(\n",
    "        self,\n",
    "        document_paths: Path | list[Path],\n",
    "        chunk_size: int = 2_000,\n",
    "        chunk_overlap: int = 500,\n",
    "    ):\n",
    "        \"\"\"Add documents to the QueryBot DocumentStore.\"\"\"\n",
    "        if isinstance(document_paths, Path):\n",
    "            document_paths = [document_paths]\n",
    "\n",
    "        for document_path in tqdm(document_paths):\n",
    "            document = magic_load_doc(document_path)\n",
    "            splitted_document = split_document(\n",
    "                document, chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "            )\n",
    "            chunks_to_add = [\n",
    "                doc\n",
    "                for doc in splitted_document\n",
    "                if doc not in self.existing_records[\"documents\"]\n",
    "            ]\n",
    "            self.extend(chunks_to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = LanceDBDocStore(table_name=\"my_table\")\n",
    "db.reset()\n",
    "\n",
    "db.append(\"hello world!\")\n",
    "db.extend([\"Hello world!\", \"hello again!\"])\n",
    "# db.table.add([{\"document\": \"Hello world!\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.retrieve(\"hello_world\", n_results=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamabot.components.docstore import LanceDBDocStore\n",
    "\n",
    "\n",
    "db = LanceDBDocStore(table_name=\"my_table\")\n",
    "db.reset()\n",
    "db.append(\"hello world!\")\n",
    "db.extend([\"hello world!\", \"hello again!\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.retrieve(\"aloha\", n_results=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamabot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
