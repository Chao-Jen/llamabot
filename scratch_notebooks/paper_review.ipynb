{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamabot import QueryBot, PromptRecorder\n",
    "from pyprojroot import here\n",
    "\n",
    "pr = PromptRecorder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = QueryBot(\n",
    "    \"You are a bot that summarizes the contents of a paper.\",\n",
    "    doc_paths=[here() / \"data/JMLR-23-0380-1.pdf\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pr:\n",
    "    bot(\n",
    "        \"Summarize this paper the way Richard Feynman would summarize it to a college student.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pr:\n",
    "    bot(\n",
    "        \"From the authors' perspective, why would breaking the co-adaptation between feature extractor and classifier can lead to better generalization?\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pr:\n",
    "    bot(\n",
    "        \"There is a term I don't get: 'point-like distributions of features for each class'. What does this mean?\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pr:\n",
    "    bot(\n",
    "        \"What computational experiments did the authors do to show that breaking the co-adaptation leads to better generalization?\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pr:\n",
    "    bot(\n",
    "        \"I don't get this term: 'approximate geodesic distance metric'. What does this mean?\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pr:\n",
    "    bot(\n",
    "        \"I don't get these terms: 'large-dataset solution' and 'small-dataset solutions'. What do these mean?\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pr:\n",
    "    bot(\"Can you describe in detail what FOCA is? and what is PoF in detail?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pr:\n",
    "    bot(\n",
    "        \"With FOCA, what is the loss function for optimizing the feature-extractor part of a deep network while keeping the classifier part undetermined?\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pr:\n",
    "    bot(\n",
    "        \"Can you explain this question (ϕ⋆ = arg min ϕ (1/nD) ∑(x,t)∈D Eθ∼Θϕ L(Cθ(Fϕ(x)), t)) in plain English? What is the intuition here?\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pr:\n",
    "    bot(\n",
    "        \"In this equation (ϕ⋆ = arg min ϕ (1/nD) ∑(x,t)∈D Eθ∼Θϕ L(Cθ(Fϕ(x)), t)), where do we get a distribution of classifier parameters from? Are they pre-trained top layers? Or are they randomly initialized neural networks?\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pr:\n",
    "    bot(\n",
    "        \"Can you describe FOCA from a procedural perspective? How does it work? What is the algorithm, written in algorithm form, with an emphasis on translating symbols into plain English?\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pr:\n",
    "    bot(\n",
    "        \"In comparing FOCA to other methods, what details in other methods did they change to make the comparison fair? For example, did they use other techniques to maximize the generalization performance of other methods, and if so, what were they?\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pr:\n",
    "    bot(\n",
    "        \"Are there logical flaws in the paper that you can identify? If so, what are they?\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pr:\n",
    "    bot(\n",
    "        \"What are the limitations of the theoretical proofs in the paper? What are the assumptions that the proofs make, explained in plain English? Are these assumptions realistic?\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pr:\n",
    "    bot(\n",
    "        \"A specific distribution of classifier parameters is assumed. What is this distribution in both mathematical form and its intuition in English?\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pr:\n",
    "    bot(\n",
    "        \"With this particular assumption, that 'the activation function satisfies a specific condition', what is that condition in mathematical form, and what is the intuition behind that condition, in plain English?\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pr:\n",
    "    bot(\n",
    "        \"What are potential caveats of this work that are not mentioned by the authors?\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pr:\n",
    "    bot(\n",
    "        \"What neural network architectures did the authors use in their experiments? Did they claim that their results generalize to other architectures? If so, what is the justification for that claim?\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pr:\n",
    "    bot(\n",
    "        \"An architecture introduced by Lee et al. (2016) for CIFAR-10 experiments, with some modifications - what modifications were introduced?\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pr:\n",
    "    bot(\n",
    "        \"The proposed methods, especially FOCA, involve the use of multiple weak classifiers during the feature-extractor training - do the authors suggest how many weak classifiers are needed?\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pr:\n",
    "    bot(\n",
    "        \"Do the authors provide practical recommendations on the minibatch size for training FOCA?\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pr:\n",
    "    bot(\n",
    "        \"Do the authors provide a link to a GitHub repository where we can examine the training code?\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pr:\n",
    "    bot(\n",
    "        \"CIFAR-10 and CIFAR-100 are image datasets. Do the authors provide evidence of FOCA outperforming other methods on non-image datasets?\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "talking_points = \"\"\"\n",
    "0. I focused on FOCA, not on PoF, as I didn't have sufficient time to cover both.\n",
    "   Also, I am a practitioner, not a theoretician, so I focused on the practical aspects of the paper.\n",
    "1. The FOCA method is logically well-motivated.\n",
    "2. I had to use GPT to translate the FOCA Approximate minimization algorithm into plain English.\n",
    "   I think the authors could have done a better job explaining the intuition behind the algorithm,\n",
    "   which would have improved the readability of the paper.\n",
    "   (To be clear, I am only saying that the algorithm wasn't presented clearly,\n",
    "   however, I think the algorithm is correct.)\n",
    "   If they want, I am providing the GPT translation of the symbols for Algorithm 1 below.\n",
    "\n",
    "<ALGORITHM BEGINS>\n",
    "1. Initialize the feature extractor parameters (ϕ) with random variables.\n",
    "2. For each iteration (t) from 1 to the total number of iterations (T):\n",
    "   a. Create a set of indices (Ic) for each class by randomly selecting 'k' samples per class.\n",
    "   b. Update the classifier parameters (θ) by minimizing the sum of sample-wise losses (L) and a regularization term for the selected samples in Ic.\n",
    "   c. Randomly select a mini-batch of size 'm' from the total number of samples (nD) for ϕ-update.\n",
    "   d. Update the feature extractor parameters (ϕ) by minimizing the average sample-wise loss (L) for the selected mini-batch, using the learning rate (η).\n",
    "3. The final feature-extractor parameters (ϕ⋆) are obtained after all iterations.\n",
    "\n",
    "In this algorithm:\n",
    "- T: total number of iterations\n",
    "- C: number of classes\n",
    "- nc(c=1,···,C): number of class-c samples\n",
    "- k: number of samples per class for θ-update\n",
    "- nD: total number of samples\n",
    "- m: mini-batch size for ϕ-update\n",
    "- η: learning rate\n",
    "<ALGORITHM ENDS>\n",
    "\n",
    "3. This next question should be addressed in the discussion.\n",
    "   Because FOCA uses a distribution over classifiers, is it possible to instead use a Bayesian neural network (BNN) for the classifier?\n",
    "   The FOCA algorithm appears to be training a feature extractor to work with the equivalent of a BNN,\n",
    "   so it seems logical to me that we should be able to use a BNN in lieu of a collection of classifiers.\n",
    "   If instead we were to train a feature extractor with a BNN, would there be speedups because we could swap minibatch-wise trainign \n",
    "   with training the parameters of a single BNN?\n",
    "4. FOCA has been shown to produce slightly superior test set performance for image data and convolutional neural networks. \n",
    "   However, I wonder about the generalizability of the method to other kinds of neural networks,\n",
    "   such as Transformers and the simpler MLP, as well as to non-image data.\n",
    "5. Related to the previous point, as a practitioner, I always consider the ease by which I am able to use a method.\n",
    "   If the method requires lots of custom code to write, I would usually steer away from it.\n",
    "   Given the claims of superior performance, and given that modern machine learning papers\n",
    "   that seek maximal impact often come with code, I think a section in the paper that shows us how to implement FOCA in code,\n",
    "   or an example of FOCA in code inside the paper, would be a good idea.\n",
    "\"\"\"\n",
    "\n",
    "with pr:\n",
    "    bot(\n",
    "        \"Thank you for helping me with the paper Q&A. \"\n",
    "        \"I am now going to prepare a suite of bullet points to write up as my review. \"\n",
    "        \"Please help me rewrite it into a coherent critique of the paper. \"\n",
    "        \"Please also try to expand on some of the points in there. \"\n",
    "        \"Here are the bullet points:\\n\\n\"\n",
    "        f\"{talking_points}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamabot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
