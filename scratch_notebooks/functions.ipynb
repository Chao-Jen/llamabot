{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polite_function():\n",
    "    \"\"\"Return a polite greeting.\"\"\"\n",
    "    return \"Hello! How are you doing?\"\n",
    "\n",
    "\n",
    "def impolite_function():\n",
    "    \"\"\"Return an impolite greeting.\"\"\"\n",
    "    return \"Hey! How are you doing?\"\n",
    "\n",
    "\n",
    "import inspect\n",
    "import typing\n",
    "\n",
    "\n",
    "def describe_function(func):\n",
    "    # Extract the signature of the function\n",
    "    signature = inspect.signature(func)\n",
    "    docstring = inspect.getdoc(func)\n",
    "\n",
    "    # Assuming the first line of the docstring is the function description\n",
    "    function_description = docstring.split(\"\\n\")[0]\n",
    "\n",
    "    # Extracting parameter information\n",
    "    parameters = {}\n",
    "    for name, param in signature.parameters.items():\n",
    "        # Assume the description is in the format: `name: description`\n",
    "        param_description = [\n",
    "            line.split(\": \")[1]\n",
    "            for line in docstring.split(\"\\n\")\n",
    "            if line.startswith(name + \":\")\n",
    "        ]\n",
    "        param_description = param_description[0] if param_description else \"\"\n",
    "\n",
    "        # Building the parameter info\n",
    "        param_type = type_to_str(param.annotation)\n",
    "        param_info = {\"description\": param_description}\n",
    "        if isinstance(param_type, dict):\n",
    "            # If the type is a dictionary (e.g., for enum), merge it with param_info\n",
    "            param_info.update(param_type)\n",
    "        else:\n",
    "            param_info[\"type\"] = param_type\n",
    "\n",
    "        parameters[name] = param_info\n",
    "\n",
    "    # Required parameters are those without default values\n",
    "    required_params = [\n",
    "        name\n",
    "        for name, param in signature.parameters.items()\n",
    "        if param.default == param.empty\n",
    "    ]\n",
    "\n",
    "    # Constructing the final description\n",
    "    result = {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": func.__name__,\n",
    "            \"description\": function_description,\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": parameters,\n",
    "                \"required\": required_params,\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Example function to describe\n",
    "def get_current_weather(\n",
    "    location: str, format: typing.Literal[\"celsius\", \"fahrenheit\"]\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Get the current weather.\n",
    "\n",
    "    :param location: The city and state, e.g. San Francisco, CA.\n",
    "    :param format: The temperature unit to use.\n",
    "    \"\"\"\n",
    "    return \"Sunny\"\n",
    "\n",
    "\n",
    "def python_type_to_json_type(python_type):\n",
    "    \"\"\"Map Python types to JSON Schema types.\"\"\"\n",
    "    type_map = {\n",
    "        str: \"string\",\n",
    "        int: \"integer\",\n",
    "        float: \"number\",\n",
    "        bool: \"boolean\",\n",
    "        list: \"array\",\n",
    "        dict: \"object\",\n",
    "        # Add more mappings as needed\n",
    "    }\n",
    "    return type_map.get(python_type, \"any\")\n",
    "\n",
    "\n",
    "def type_to_str(type_hint):\n",
    "    \"\"\"Convert type hints to JSON-friendly string representations.\"\"\"\n",
    "    if type_hint == inspect.Parameter.empty:\n",
    "        return \"any\"\n",
    "    if getattr(type_hint, \"__origin__\", None) is typing.Literal:\n",
    "        # Handling typing.Literal to convert it to JSON enum format\n",
    "        return {\"enum\": list(type_hint.__args__)}\n",
    "    if hasattr(type_hint, \"__origin__\"):  # For handling generic types like List[str]\n",
    "        origin = type_hint.__origin__\n",
    "        if origin is list:\n",
    "            # Assuming only simple types like List[str], not nested like List[List[str]]\n",
    "            args = type_hint.__args__[0]\n",
    "            return f\"array of {python_type_to_json_type(args)}\"\n",
    "        # Handle other generic types (like Dict, Tuple) here as needed\n",
    "    return python_type_to_json_type(type_hint)\n",
    "\n",
    "\n",
    "# Using the function\n",
    "# print(describe_function(polite_function))\n",
    "tools = [\n",
    "    describe_function(polite_function),\n",
    "    describe_function(impolite_function),\n",
    "    describe_function(get_current_weather),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_function(get_current_weather)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdfasdfasdfasdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamabot.components.tools import Tools\n",
    "\n",
    "\n",
    "tools = Tools(polite_function, impolite_function, get_current_weather)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a polite robot.\"},\n",
    "        {\"role\": \"system\", \"content\": \"Always pick the polite function.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the weather in my area, Cambridge, MA?\",\n",
    "        },\n",
    "    ],\n",
    "    tools=tools.schemas(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.choices[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from llamabot.components.messages import BaseMesasge\n",
    "\n",
    "\n",
    "class History:\n",
    "    def __init__(self):\n",
    "        self.messages: list[BaseMesasge] = []\n",
    "\n",
    "    def append(self, message):\n",
    "        self.messages.append(message)\n",
    "\n",
    "    def retrieve(self, token_budget: int, model_name: str):\n",
    "        \"\"\"Retrieve messages from the history up to the token budget.\"\"\"\n",
    "        tokenizer = tiktoken.encoding_for_model(model_name)\n",
    "        tokens = 0\n",
    "        messages = []\n",
    "        for message in self.messages:\n",
    "            tokens += tokenizer(message).input_ids.shape[1]\n",
    "            if tokens > token_budget:\n",
    "                # trim the message until it fits.\n",
    "                message = message[: -(tokens - token_budget)]\n",
    "            messages.append(message)\n",
    "        return messages\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.messages[index]\n",
    "\n",
    "\n",
    "class RAGHistory:\n",
    "    def __init__(self):\n",
    "        self.messages: list[BaseMesasge] = []\n",
    "\n",
    "    def append(self, message):\n",
    "        # Embed the message content inside an in-memory vector store for RAG.\n",
    "        self.messages.append(message)\n",
    "\n",
    "    # def retrieve(self, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools(response.choices[0].message.tool_calls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = SystemMessage(content=\"You are a polite robot.\")\n",
    "message.model_dump()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamabot import SimpleBot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = SimpleBot(\"You are a polite robot.\")\n",
    "bot(\"What is the weather in my area, Cambridge, MA?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AIMessage(content=response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.choices[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools(response.choices[0].message.tool_calls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response.choices[0].message\n",
    "tool_calls = response_message.tool_calls\n",
    "\n",
    "# if tool_calls:\n",
    "#     # Step 3: call the function\n",
    "#     # Note: the JSON response may not always be valid; be sure to handle errors\n",
    "#     messages.append(response_message)  # extend conversation with assistant's reply\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_calls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = available_functions[tool_calls[0].function.name]\n",
    "import json\n",
    "\n",
    "func_kwargs = json.loads(tool_calls[0].function.arguments)\n",
    "func(**func_kwargs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamabot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
