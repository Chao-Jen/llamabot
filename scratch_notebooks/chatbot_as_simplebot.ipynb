{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An experiment rewriting the ChatBot as the executor of a SimpleBot instead.\n",
    "# Building an organism here...????\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamabot import ChatBot\n",
    "\n",
    "\n",
    "bot = ChatBot(\"You are a chatbot.\", session_name=\"chatbot\")\n",
    "bot(\"hey there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(bot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamabot import SimpleBot\n",
    "from llamabot.components.history import History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimpleBot rewrite using the OpenAI API and Ollama API\n",
    "model_name = \"mistral:7b\"\n",
    "from openai import OpenAI\n",
    "from llamabot.bot.model_dispatcher import ollama_model_keywords\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def make_client(model_name):\n",
    "    \"\"\"Use OpenAI, or else use LiteLLM to interface with local LLMs.\n",
    "\n",
    "    You will need to install LiteLLM in order to access local LLMs.\n",
    "    \"\"\"\n",
    "    client = OpenAI()\n",
    "    if model_name.split(\":\")[0] in ollama_model_keywords():\n",
    "        client = OpenAI(base_url=\"http://0.0.0.0:8000\", api_key=\"dummy\")\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamabot.components.messages import (\n",
    "    HumanMessage,\n",
    "    AIMessage,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamabot.config import default_language_model\n",
    "\n",
    "\n",
    "from llamabot.bot.simplebot import SimpleBot\n",
    "\n",
    "\n",
    "bot = SimpleBot(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    system_prompt=\"You are a helpful and humorous llama.\",\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "response = bot(\"Say yes or no.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from llamabot.bot.chatbot import ChatBot\n",
    "from llamabot.components.history import RAGHistory, History\n",
    "\n",
    "bot = ChatBot(\n",
    "    system_prompt=\"You are a very helpful Llama.\",\n",
    "    session_name=\"testing\",\n",
    "    chat_history_class=History,\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "bot(\"Hey there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot(\"How are you doing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot(\"What's going on in the world today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot(\"What are you doing right now?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot(\"What do you think about messages?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot(\"Make up a joke.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now let's compose querybot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamabot.components.docstore import DocumentStore\n",
    "from pathlib import Path\n",
    "from llamabot.doc_processor import magic_load_doc, split_document\n",
    "from llamabot.components.messages import (\n",
    "    RetrievedMessage,\n",
    "    retrieve_messages_up_to_budget,\n",
    ")\n",
    "from llamabot.bot.model_tokens import model_context_window_sizes, DEFAULT_TOKEN_BUDGET\n",
    "\n",
    "\n",
    "class QueryBot:\n",
    "    \"\"\"QueryBot is a bot that uses simple RAG to answer questions about a document.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        system_prompt: str,\n",
    "        document_paths: Path | list[Path],\n",
    "        collection_name: str,\n",
    "        temperature: float = 0.0,\n",
    "        model_name: str = default_language_model(),\n",
    "        stream=True,\n",
    "    ):\n",
    "        self.bot = SimpleBot(\n",
    "            system_prompt=system_prompt,\n",
    "            temperature=temperature,\n",
    "            model_name=model_name,\n",
    "            stream=stream,\n",
    "        )\n",
    "        self.document_store = DocumentStore(collection_name=collection_name)\n",
    "        self.add_documents(document_paths=document_paths)\n",
    "        self.response_budget = 2_000\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def add_documents(\n",
    "        self,\n",
    "        document_paths: Path | list[Path],\n",
    "        chunk_size: int = 2_000,\n",
    "        chunk_overlap: int = 500,\n",
    "    ):\n",
    "        if isinstance(document_paths, Path):\n",
    "            document_paths = [document_paths]\n",
    "\n",
    "        for document_path in document_paths:\n",
    "            document = magic_load_doc(document_path)\n",
    "            splitted_document = split_document(\n",
    "                document, chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "            )\n",
    "            splitted_document = [doc.text for doc in splitted_document]\n",
    "            self.document_store.extend(splitted_document)\n",
    "\n",
    "    def __call__(self, query, n_results: int = 10) -> AIMessage:\n",
    "        messages = []\n",
    "\n",
    "        context_budget = model_context_window_sizes.get(\n",
    "            self.model_name, DEFAULT_TOKEN_BUDGET\n",
    "        )\n",
    "        retrieved = retrieve_messages_up_to_budget(\n",
    "            messages=[\n",
    "                RetrievedMessage(content=chunk)\n",
    "                for chunk in self.document_store.retrieve(query, n_results=n_results)\n",
    "            ],\n",
    "            character_budget=context_budget - self.response_budget,\n",
    "        )\n",
    "        messages.extend(retrieved)\n",
    "        messages.append(HumanMessage(content=query))\n",
    "        response: str = self.bot.generate_response(messages)\n",
    "        return AIMessage(content=response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo-1106\",\n",
    "    response_format={\"type\": \"json_object\"},\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant designed to output JSON.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "    ],\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from llamabot.config import default_language_model\n",
    "\n",
    "from llamabot.prompt_manager import prompt\n",
    "from llamabot.bot.simplebot import SimpleBot\n",
    "\n",
    "\n",
    "@prompt\n",
    "def kgbot_sysprompt() -> str:\n",
    "    \"\"\"You are an expert ontologist. You are tasked with taking in a chunk of text\n",
    "    and extracting as many relationships as possible from that text\n",
    "    without extrapolating any relationships that are not explicitly stated.\n",
    "    If you encounter a bibliography entry, you should ignore it.\n",
    "\n",
    "    For each relationship, return a JSON according to the following schema:\n",
    "\n",
    "    {\n",
    "        \"subject\": \"string\",\n",
    "        \"predicate\": \"string\",\n",
    "        \"object\": \"string\",\n",
    "        \"evidence\": \"quote from the text\"\n",
    "    }\n",
    "\n",
    "    You should return it as a list of dictionaries, like so:\n",
    "\n",
    "    [\n",
    "        {\"subject\": \"string\", \"predicate\": \"string\", \"object\": \"string\", \"evidence\": \"string\"},\n",
    "        {\"subject\": \"string\", \"predicate\": \"string\", \"object\": \"string\", \"evidence\": \"string\"},\n",
    "        {\"subject\": \"string\", \"predicate\": \"string\", \"object\": \"string\", \"evidence\": \"string\"},\n",
    "        ...\n",
    "    ]\n",
    "\n",
    "    If the entire chunk is comprised of bibliographic entries, then return an empty list.\n",
    "    Do not justify your actions.\n",
    "    Ensure that for the \"evidence\" field you are quoting the text verbatim.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "# json_cleaner = SimpleBot(\"You are a JSON cleaner. You will be provided with dirty JSON, and your task is to clean it up to be valid JSON.\", model_name=\"mistral/mistral-tiny\")\n",
    "\n",
    "\n",
    "class KGBot:\n",
    "    \"\"\"KGBot is the Knowledge Graph bot.\n",
    "\n",
    "    It takes in a chunk of text and returns a JSON of triplets.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        system_prompt=kgbot_sysprompt(),\n",
    "        temperature: float = 0.0,\n",
    "        model_name: str = default_language_model(),\n",
    "        stream: bool = True,\n",
    "    ):\n",
    "        self.bot = SimpleBot(\n",
    "            system_prompt=system_prompt,\n",
    "            temperature=temperature,\n",
    "            model_name=model_name,\n",
    "            stream=stream,\n",
    "        )\n",
    "\n",
    "    def __call__(self, query: str) -> dict:\n",
    "        \"\"\"Call the bot with a query and return a JSON of triplets.\"\"\"\n",
    "        response = self.bot(query)\n",
    "        return json.loads(response.content.strip(\"```json\").strip(\"```\"))\n",
    "\n",
    "\n",
    "kgbot = KGBot(model_name=\"mistral/mistral-tiny\")\n",
    "# kgbot(\"Alice is married to Bob. Alice lives in London. Bob lives in Paris. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamabot.doc_processor import magic_load_doc, split_document\n",
    "from pyprojroot import here\n",
    "\n",
    "document = magic_load_doc(here() / \"data/codonbert.pdf\")\n",
    "chunks = split_document(document, chunk_size=1_024, chunk_overlap=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamabot.bot.qabot import DocQABot\n",
    "\n",
    "\n",
    "qabot = DocQABot(collection_name=\"codonbert\")\n",
    "# qabot(\"How did the authors show that CodonBERT learns the genetic code?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qabot(\"How did the authors show that CodonBERT learns the genetic code?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamabot import SimpleBot\n",
    "from llamabot.prompt_manager import prompt\n",
    "import json\n",
    "\n",
    "\n",
    "@prompt\n",
    "def jeopardy_bot_sysprompt():\n",
    "    \"\"\"\n",
    "    You are an expert at taking texts and constructing questions from them.\n",
    "    You will be given a text.\n",
    "    Extract as many question-and-answer pairs.\n",
    "    Each answer may have multiple questions; be sure to cover as many as possible.\n",
    "    Return a JSON array of the following schema:\n",
    "\n",
    "    {\n",
    "        \"questions_and_answers\": [\n",
    "            {\n",
    "                \"question\": \"string\",\n",
    "                \"answer\": \"string\",\n",
    "            },\n",
    "            ...\n",
    "        ]\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "jeopardy_bot = SimpleBot(\n",
    "    system_prompt=jeopardy_bot_sysprompt(),\n",
    "    # model_name=\"gpt-3.5-turbo-1106\",\n",
    "    # model_name=\"ollama/tinyllama\",\n",
    "    model_name=\"mistral/mistral-tiny\",\n",
    "    json_mode=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "\n",
    "litellm.set_verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jeopardy_bot(\"The capital of France is Paris.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json.loads(jeopardy_bot(\"The capital of France is Paris.\").content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can incorporate Jeopardy Bot into the embedding process. This is how we will do it. Store the questions in the vector DB under the collection `{collection_name}_questions`, and within the metadata, store the hash of the document. Then, when we get asked a question, we do a vector similarity search against `{collection_name}_questions`, get the top 1 or 2 relevant results, and then use the hash to retrieve the original document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_store = DocumentStore(collection_name=\"codonbert_questions\")\n",
    "document_store = DocumentStore(collection_name=\"codonbert\")\n",
    "question_store.reset()\n",
    "document_store.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamabot.components.docstore import DocumentStore\n",
    "from hashlib import sha256\n",
    "\n",
    "for chunk in chunks:\n",
    "    doc_id = sha256(chunk.text.encode()).hexdigest()\n",
    "    document_store.append(chunk.text, metadata=dict(doc_id=doc_id))\n",
    "    q_and_a = json.loads(jeopardy_bot(chunk.text).content)\n",
    "    for q_and_a in q_and_a[\"questions_and_answers\"]:\n",
    "        q_a_concat = f\"Q: {q_and_a['question']} A: {q_and_a['answer']}\"\n",
    "        question_store.append(q_a_concat, metadata=dict(parent_doc=doc_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_and_as = question_store.retrieve(\"What is CodonBERT?\")\n",
    "q_and_as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"What papers did this paper cite?\"\n",
    "result = question_store.collection.query(query_texts=query_text, n_results=20)\n",
    "result[\"metadatas\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique parent_doc IDs:\n",
    "# preserve order\n",
    "parent_doc_ids = []\n",
    "for metadata in result[\"metadatas\"][0]:\n",
    "    if metadata[\"parent_doc\"] not in parent_doc_ids:\n",
    "        parent_doc_ids.append(metadata[\"parent_doc\"])\n",
    "# parent_doc_ids = set([metadata[\"parent_doc\"] for metadata in result[\"metadatas\"][0]])\n",
    "parent_doc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = document_store.collection.query(\n",
    "    query_texts=query_text, where={\"doc_id\": {\"$in\": list(parent_doc_ids)}}, n_results=3\n",
    ")\n",
    "len(results[\"documents\"][0])\n",
    "relevant_documents = results[\"documents\"][0]\n",
    "relevant_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, compose the final prompt that includes the Q&A results and the retrieved documents\n",
    "\n",
    "\n",
    "@prompt\n",
    "def q_and_a_prompt(query, q_and_a_results, relevant_documents):\n",
    "    \"\"\"Q&A Results:\n",
    "\n",
    "        {{ q_and_a_results }}\n",
    "\n",
    "    Relevant documents:\n",
    "\n",
    "        {{ relevant_documents }}\n",
    "\n",
    "    Query:\n",
    "\n",
    "        {{ query }}\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "response_bot = SimpleBot(\n",
    "    \"Based on Q&A results and relevant documents, please answer the query.\"\n",
    ")\n",
    "response_bot(q_and_a_prompt(query_text, q_and_as, relevant_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatBot as a mixin\n",
    "from llamabot.bot.simplebot import SimpleBot\n",
    "from llamabot.components.history import History\n",
    "from llamabot.components.messages import HumanMessage, AIMessage\n",
    "\n",
    "\n",
    "class ChatBot(SimpleBot, History):\n",
    "    def __init__(\n",
    "        self,\n",
    "        system_prompt: str,\n",
    "        session_name: str,\n",
    "        temperature=0.0,\n",
    "        model_name=\"mistral/mistral-tiny\",\n",
    "        stream=True,\n",
    "        response_budget=2_000,\n",
    "    ):\n",
    "        SimpleBot.__init__(\n",
    "            self,\n",
    "            system_prompt=system_prompt,\n",
    "            temperature=temperature,\n",
    "            model_name=model_name,\n",
    "            stream=stream,\n",
    "        )\n",
    "        History.__init__(self, session_name=session_name)\n",
    "        self.model_name = model_name\n",
    "        self.response_budget = response_budget\n",
    "        self.session_name = session_name\n",
    "\n",
    "    def __call__(self, message: str) -> AIMessage:\n",
    "        \"\"\"Call the ChatBot.\n",
    "\n",
    "        :param human_message: The human message to use.\n",
    "        :return: The response to the human message, primed by the system prompt.\n",
    "        \"\"\"\n",
    "        human_message = HumanMessage(content=message)\n",
    "        history = self.retrieve(\n",
    "            query=human_message, character_budget=self.response_budget\n",
    "        )\n",
    "        messages = [self.system_prompt] + history + [human_message]\n",
    "        response = self.generate_response(messages)\n",
    "        # autorecord(human_message, response.content)\n",
    "\n",
    "        self.append(human_message)\n",
    "        self.append(response)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = ChatBot(\"You are a non-chatty bot.\", session_name=\"chat_session\")\n",
    "chatbot(\"Hello!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to make the document store now\n",
    "from llamabot.components.docstore import DocumentStore\n",
    "\n",
    "document_store = DocumentStore(collection_name=\"codonbert_documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kgbot(chunks[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets = []\n",
    "for chunk in chunks:\n",
    "    triplets.extend(kgbot(chunk.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Draw a sample\n",
    "# rng = torch.Generator(device=\"cuda\")\n",
    "# rng.manual_seed(789001)\n",
    "\n",
    "# sequence = generator(\"Give me a character description\", rng=rng)\n",
    "# print(sequence)\n",
    "# # {\n",
    "# #   \"name\": \"clerame\",\n",
    "# #   \"age\": 7,\n",
    "# #   \"armor\": \"plate\",\n",
    "# #   \"weapon\": \"mace\",\n",
    "# #   \"strength\": 4171\n",
    "# # }\n",
    "\n",
    "# sequence = generator(\"Give me an interesting character description\", rng=rng)\n",
    "# print(sequence)\n",
    "# # {\n",
    "# #   \"name\": \"piggyback\",\n",
    "# #   \"age\": 23,\n",
    "# #   \"armor\": \"chainmail\",\n",
    "# #   \"weapon\": \"sword\",\n",
    "# #   \"strength\": 0\n",
    "# # }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyprojroot import here\n",
    "\n",
    "qb = QueryBot(\n",
    "    \"You are an expert in answering questions about a paper that you will be provided.\",\n",
    "    collection_name=\"FOCA_paper\",\n",
    "    document_paths=here() / \"data\" / \"JMLR-23-0380-1.pdf\",\n",
    "    model_name=\"mistral/mistral-medium\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qb(\"What is POF?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamabot.components.messages import HumanMessage, SystemMessage\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "\n",
    "docstore = DocumentStore(collection_name=\"querybot_stuff\")\n",
    "history = History(session_name=\"querybot_stuff\")\n",
    "\n",
    "bot = SimpleBot(\n",
    "    \"You are an expert at reading papers.\", model_name=\"mistral/mistral-medium\"\n",
    ")\n",
    "\n",
    "\n",
    "doc_path = here() / \"data\" / \"JMLR-23-0380-1.pdf\"\n",
    "document = magic_load_doc(doc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docstore.append(document[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_document = split_document(document[0], chunk_size=4_000, chunk_overlap=200)\n",
    "len(splitted_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docstore.collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in splitted_document:\n",
    "    docstore.append(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize by doing a summary of each chunk\n",
    "# splitted_document[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamabot.prompt_manager import prompt\n",
    "from llamabot.prompt_library.zotero import docbot_sysprompt, paper_summary\n",
    "\n",
    "\n",
    "@prompt\n",
    "def summarization_bot_prompt(text_to_summarize):\n",
    "    \"\"\"Here is the text to summarize:\n",
    "\n",
    "    {{ text_to_summarize }}\n",
    "\n",
    "    Your summary should not be a mere regurgitation of the abstract.\n",
    "    Rather, your summary should highlight the key findings,\n",
    "    methodology, and implications.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "summarization_bot = SimpleBot(\n",
    "    system_prompt=docbot_sysprompt(),\n",
    "    model_name=\"mistral/mistral-medium\",\n",
    ")\n",
    "\n",
    "new_summary = summarization_bot(summarization_bot_prompt(splitted_document[0].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Help define certain statements\n",
    "\n",
    "\n",
    "@prompt\n",
    "def define(term, text):\n",
    "    \"\"\"Here is a text: {{ text }}\n",
    "\n",
    "    Please help me see if `text` defines the term {{ term }}.\n",
    "\n",
    "    Based on that infrmation, fill out the following JSON for me:\n",
    "\n",
    "    {\n",
    "        \"term\": \"term\",\n",
    "        \"definition\": \"definition\",\n",
    "        \"context\": \"exact quote from text\",\n",
    "        \"source\": \"source\"\n",
    "    }\n",
    "\n",
    "    If the term is not defined in the text, then return None as values as follows:\n",
    "\n",
    "    {\n",
    "        \"term\": \"term\",\n",
    "        \"definition\": None,\n",
    "        \"context\": None,\n",
    "        \"source\": None\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "@prompt\n",
    "def key_terms(text):\n",
    "    \"\"\"\n",
    "    Here is a text: {{ text }}\n",
    "\n",
    "    Within the text, identify key terms that have definitions present in the text.\n",
    "\n",
    "    Then, for each term, fill out the following JSON for me:\n",
    "\n",
    "    {\n",
    "        \"term\": \"term\",\n",
    "        \"definition\": \"definition\",\n",
    "        \"context\": \"exact quote from text\",\n",
    "        \"source\": \"source\"\n",
    "    }\n",
    "\n",
    "    Return an array of JSONs.\n",
    "    Ensure that each term is only defined once.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "definition_bot = SimpleBot(\n",
    "    system_prompt=\"You are a bot that searches texts for definitions of terms.\",\n",
    "    model_name=\"mistral/mistral-tiny\",\n",
    ")\n",
    "\n",
    "\n",
    "definition_bot(define(\"FOCA\", splitted_document[0].text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a knowledge graph of the document. Schema:\n",
    "\n",
    "- (hash node)--is hash of--(text)\n",
    "- (definition)--is defined in--(hash)\n",
    "- (concept)--relates to--(definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from hashlib import sha256\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "for doc in splitted_document:\n",
    "    # add node, node = hash, attribute text=doc.text\n",
    "    G.add_node(\n",
    "        sha256(doc.text.encode(\"utf-8\")).hexdigest(), text=doc.text, node_type=\"text\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_definitions = {}\n",
    "import json\n",
    "\n",
    "for doc in splitted_document:\n",
    "    try:\n",
    "        parsed_definitions = definition_bot(key_terms(doc.text)).content\n",
    "        definitions = json.loads(parsed_definitions)\n",
    "        for definition in definitions:\n",
    "            paper_definitions[definition[\"term\"]] = definition\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_definitions.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamabot.components.retrieve_messages_up_to_budget import (\n",
    "    retrieve_messages_up_to_budget,\n",
    ")\n",
    "\n",
    "query = \"Based on the content below from a paper, please summarize the paper for me.\"\n",
    "results = docstore.retrieve(query, n_results=50)\n",
    "# results[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding that you use affects retrieval, but not synthesis.\n",
    "So that means we can use an entirely locally hosted embedding model,\n",
    "such as Sentence Transformers,\n",
    "or we can use a remotely hosted embedding model,\n",
    "such as OpenAI's embeddings API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am also going to see how much I can decouple from llama_index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.auto import partition\n",
    "from unstructured.cleaners.core import (\n",
    "    clean,\n",
    "    replace_unicode_quotes,\n",
    "    group_broken_paragraphs,\n",
    ")\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "from pyprojroot import here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb.utils import embedding_functions\n",
    "import os\n",
    "\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"), model_name=\"text-embedding-ada-002\"\n",
    ")\n",
    "try:\n",
    "    client.delete_collection(\"dshiring\")\n",
    "except ValueError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.create_collection(\"dshiring\", get_or_create=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = partition(here() / \"data\" / \"dshiring.pdf\")\n",
    "chunks = chunk_by_title(elements, new_after_n_chars=5_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(elements: list, cleaning_funcs: list):\n",
    "    cleaned_elements = []\n",
    "    for element in elements:\n",
    "        for cleaning_func in cleaning_funcs:\n",
    "            if isinstance(element, str):\n",
    "                element = cleaning_func(element)\n",
    "            else:\n",
    "                element = cleaning_func(element.text)\n",
    "        cleaned_elements.append(element)\n",
    "    return cleaned_elements\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "clean = partial(\n",
    "    clean, bullets=True, extra_whitespace=True, dashes=True, trailing_punctuation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_elements = cleanup(\n",
    "    chunks, cleaning_funcs=[replace_unicode_quotes, clean, group_broken_paragraphs]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_elements[148]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_texts = list(set([c for c in cleaned_elements]))\n",
    "ids = [sha256(c.encode()).hexdigest() for c in chunk_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunk_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_texts[142]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_bot = SimpleBot(\n",
    "    \"You are a knowledge assistant. I will give you a broad topic that I am interested in. You will return for me example (subject, predicate, object) keywords that are relevant for that field.\"\n",
    ")\n",
    "keyword_bot(\"Data science\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_texts[145]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_bot = SimpleBot(\n",
    "    \"You are a knowledge parsing expert bot. \"\n",
    "    \"You accept chunks of texts and return JSON-formatted property graph information. \"\n",
    "    \"Do not lift from the text verbatim. \"\n",
    "    \"Ensure that the top-level of the JSON is always an entity. \"\n",
    "    \"Check your answers thrice before returning them, ensuring accuracy. \"\n",
    "    \"Then, format the JSON into triplets.\",\n",
    "    model_name=\"gpt-4\",\n",
    ")\n",
    "kg_bot(chunk_texts[145])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(documents=chunk_texts, ids=ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.get()[\"documents\"][-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection.query(\n",
    "    query_texts=\"What does Monica say we need to be prepared for data science?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"documents\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in results[\"documents\"][0]:\n",
    "    print(len(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Design choices:\n",
    "\n",
    "- There is only one ChromaDB database by default, unless user specify otherwise.\n",
    "- Each collection of documents is its own collection within the ChromaDB database.\n",
    "- User gets to name that collection of documents, so this is a required argument.\n",
    "  - Doing so saves us a ton of complexity in inferring what constitutes a collection in the user's mind.\n",
    "  - Also lets users have control over memorable names for collections of documents.\n",
    "- Embedding model is specified per collection, defaults to Sentence Transformers, which seems to be good enough and, crucially, _free and local_.\n",
    "\n",
    "RAGHistory can be built on top of this. Every chat session gets its own collection prefixed with `chat-<date>`, and when the user asks a question, we query that chat history collection for the most relevant context, and then stuff that context into the prompt for SimpleBot to generate a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llamabot import QueryBot\n",
    "\n",
    "\n",
    "class QueryBot:\n",
    "    def __init__(\n",
    "        self,\n",
    "        system_prompt,\n",
    "        collection_name: str,\n",
    "        temperature=0.0,\n",
    "        model_name=default_language_model(),\n",
    "        streaming=True,\n",
    "        response_budget=2_000,\n",
    "        db_path: str = str(Path.home() / \".llamabot\" / \"chroma.db\"),\n",
    "    ):\n",
    "        self.chroma_client = chromadb.PersistentClient(path=db_path)\n",
    "        self.collection = self.chroma_client.create_collection(\n",
    "            collection_name, get_or_create=True\n",
    "        )\n",
    "\n",
    "        self.bot = SimpleBot(\n",
    "            system_prompt=system_prompt,\n",
    "            temperature=temperature,\n",
    "            model_name=model_name,\n",
    "            streaming=streaming,\n",
    "        )\n",
    "        self.model_name = model_name\n",
    "        self.chat_history: History = History()\n",
    "        self.response_budget = response_budget\n",
    "\n",
    "    def __call__(self, human_message: str) -> AIMessage:\n",
    "        self.chat_history.append(HumanMessage(content=human_message))\n",
    "        history = self.chat_history.retrieve(character_budget=2_000)\n",
    "        messages = [self.bot.system_prompt] + history\n",
    "        response = self.bot.generate_response([m.model_dump() for m in messages])\n",
    "        self.chat_history.append(response)\n",
    "        return response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamabot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
